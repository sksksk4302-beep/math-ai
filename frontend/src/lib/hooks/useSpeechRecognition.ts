import { useState, useCallback, useRef, useEffect } from 'react';
import { normalizeKoreanNumber } from '../utils/korean';

interface UseSpeechRecognitionProps {
    onResult: (number: string) => void;
}

export const useSpeechRecognition = ({ onResult }: UseSpeechRecognitionProps) => {
    const [isListening, setIsListening] = useState(false);
    const [isProcessingStt, setIsProcessingStt] = useState(false);
    const streamRef = useRef<MediaStream | null>(null);
    const recognitionRef = useRef<any>(null);

    // 1. Ïª¥Ìè¨ÎÑåÌä∏ ÎßàÏö¥Ìä∏ Ïãú ÎßàÏù¥ÌÅ¨ Ïä§Ìä∏Î¶º ÎØ∏Î¶¨ ÌôïÎ≥¥ (Warm-up)
    useEffect(() => {
        const initStream = async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                streamRef.current = stream;
                console.log("üé§ Microphone stream initialized");
            } catch (e) {
                console.error("Microphone access denied or not available:", e);
            }
        };

        initStream();

        // Ïñ∏ÎßàÏö¥Ìä∏ Ïãú Ïä§Ìä∏Î¶º Ï†ïÎ¶¨
        return () => {
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
                streamRef.current = null;
            }
            if (recognitionRef.current) {
                recognitionRef.current.abort();
                recognitionRef.current = null;
            }
        };
    }, []);

    const stopListening = useCallback(() => {
        if (recognitionRef.current) {
            recognitionRef.current.abort(); // stop() ÎåÄÏã† abort()Í∞Ä Îçî ÌôïÏã§ÌïòÍ≤å Ï§ëÎã®
            recognitionRef.current = null;
        }
        setIsListening(false);
    }, []);

    const handleVoiceRecord = useCallback(async () => {
        setIsListening(true);
        try {
            // Ïä§Ìä∏Î¶º Ïû¨ÏÇ¨Ïö© ÎòêÎäî ÏÉàÎ°ú ÏöîÏ≤≠
            let stream = streamRef.current;
            if (!stream || !stream.active) {
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                streamRef.current = stream;
            }

            const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
            const audioChunks: Blob[] = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) audioChunks.push(event.data);
            };

            mediaRecorder.onstop = async () => {
                setIsProcessingStt(true);
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                const formData = new FormData();
                formData.append('file', audioBlob, 'recording.webm');

                try {
                    const apiUrl = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000';
                    const res = await fetch(`${apiUrl}/stt`, {
                        method: 'POST',
                        body: formData,
                        cache: 'no-store'
                    });
                    const data = await res.json();
                    if (data.number) {
                        onResult(data.number);
                    }
                } catch (e) {
                    console.error("STT Failed:", e);
                } finally {
                    setIsProcessingStt(false);
                    setIsListening(false);
                    // Ïä§Ìä∏Î¶ºÏùÑ Îã´ÏßÄ ÏïäÍ≥† Ïú†ÏßÄÌï® (Ïû¨ÏÇ¨Ïö© ÏúÑÌï¥)
                }
            };

            mediaRecorder.start();

            setTimeout(() => {
                if (mediaRecorder.state === 'recording') {
                    mediaRecorder.stop();
                }
            }, 3500);

        } catch (e) {
            console.error("Mic access denied:", e);
            setIsListening(false);
            alert("ÎßàÏù¥ÌÅ¨ Í∂åÌïúÏù¥ ÌïÑÏöîÌï¥Ïöî! ÏÑ§Ï†ïÏóêÏÑú ÌóàÏö©Ìï¥Ï£ºÏÑ∏Ïöî. üé§");
        }
    }, [onResult]);

    const startListening = useCallback(() => {
        if (isListening || isProcessingStt) return;

        // Í∏∞Ï°¥ Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ïÎ¶¨
        if (recognitionRef.current) {
            recognitionRef.current.abort();
        }

        if ('webkitSpeechRecognition' in window) {
            const recognition = new (window as any).webkitSpeechRecognition();
            recognitionRef.current = recognition;

            recognition.lang = 'ko-KR';
            recognition.continuous = false;
            recognition.interimResults = false;

            recognition.onstart = () => {
                setIsListening(true); // Î™ÖÏãúÏ†Å ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            };

            recognition.onend = () => {
                // ÌòÑÏû¨ Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä ÎßûÎäîÏßÄ ÌôïÏù∏ (Race Condition Î∞©ÏßÄ)
                if (recognitionRef.current === recognition) {
                    setIsListening(false);
                    recognitionRef.current = null;
                }
            };

            recognition.onresult = (event: any) => {
                const transcript = event.results[0][0].transcript;
                console.log("Mic Transcript:", transcript);

                const number = normalizeKoreanNumber(transcript);

                if (number) {
                    onResult(number);
                }
            };

            recognition.onerror = (event: any) => {
                console.error("Speech recognition error", event.error);

                // ÌòÑÏû¨ Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä ÎßûÎäîÏßÄ ÌôïÏù∏
                if (recognitionRef.current === recognition) {
                    setIsListening(false);
                    recognitionRef.current = null;
                }

                if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                    handleVoiceRecord(); // Ìè¥Î∞±
                }
            };

            try {
                recognition.start();
            } catch (e) {
                console.error("Mic start error:", e);
                setIsListening(false);
                handleVoiceRecord();
            }
        } else {
            handleVoiceRecord();
        }
    }, [isListening, isProcessingStt, onResult, handleVoiceRecord]);

    return {
        isListening,
        isProcessingStt,
        startListening,
        stopListening
    };
};
