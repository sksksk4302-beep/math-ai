import { useState, useCallback, useRef, useEffect, useReducer } from 'react';
import { normalizeKoreanNumber } from '../utils/korean';

interface UseSpeechRecognitionProps {
    onResult: (number: string) => void;
}

export const useSpeechRecognition = ({ onResult }: UseSpeechRecognitionProps) => {
    // âœ… UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ forceUpdate ë©”ì»¤ë‹ˆì¦˜
    const [, forceUpdate] = useReducer(x => x + 1, 0);

    // âœ… ìƒíƒœë¥¼ refë¡œ ê´€ë¦¬ (startListeningì„ stableí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•¨)
    const isListeningRef = useRef(false);
    const isProcessingSttRef = useRef(false);

    // Fallbackìš© ìŠ¤íŠ¸ë¦¼ (webkitSpeechRecognition ì‹¤íŒ¨ ì‹œì—ë§Œ ì‚¬ìš©)
    const streamRef = useRef<MediaStream | null>(null);
    const recognitionRef = useRef<any>(null);

    // âœ… ì½œë°± í•¨ìˆ˜ë¥¼ Refì— ë‹´ì•„ ìµœì‹  ìƒíƒœ ìœ ì§€ (Closure ë¬¸ì œ í•´ê²°)
    const onResultRef = useRef(onResult);

    // âœ… ì¤‘ë³µ ì‹œì‘ ë°©ì§€ ê°€ë“œ
    const isStartingRef = useRef(false);

    useEffect(() => {
        onResultRef.current = onResult;
    }, [onResult]);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    useEffect(() => {
        return () => {
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
                streamRef.current = null;
            }
            if (recognitionRef.current) {
                recognitionRef.current.abort();
                recognitionRef.current = null;
            }
        };
    }, []);

    const setIsListening = useCallback((value: boolean) => {
        isListeningRef.current = value;
        forceUpdate();
    }, []);

    const setIsProcessingStt = useCallback((value: boolean) => {
        isProcessingSttRef.current = value;
        forceUpdate();
    }, []);

    const stopListening = useCallback(() => {
        if (recognitionRef.current) {
            recognitionRef.current.abort();
            recognitionRef.current = null;
        }
        setIsListening(false);
        isStartingRef.current = false;

        // Fallback ìŠ¤íŠ¸ë¦¼ë„ ì •ë¦¬
        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
        }
    }, [setIsListening]);

    // 2. Fallback: ì§ì ‘ ë…¹ìŒí•´ì„œ ì„œë²„ë¡œ ì „ì†¡ (Web Speech API ë¯¸ì§€ì›/ì˜¤ë¥˜ ì‹œ)
    const handleVoiceRecord = useCallback(async () => {
        setIsListening(true);
        try {
            let stream = streamRef.current;
            if (!stream || !stream.active) {
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                streamRef.current = stream;
            }

            const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
            const audioChunks: Blob[] = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) audioChunks.push(event.data);
            };

            mediaRecorder.onstop = async () => {
                setIsProcessingStt(true);
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                const formData = new FormData();
                formData.append('file', audioBlob, 'recording.webm');

                try {
                    const apiUrl = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000';
                    const res = await fetch(`${apiUrl}/stt`, {
                        method: 'POST',
                        body: formData,
                        cache: 'no-store'
                    });
                    const data = await res.json();
                    if (data.number) {
                        onResultRef.current(data.number);
                    }
                } catch (e) {
                    console.error("STT Failed:", e);
                } finally {
                    setIsProcessingStt(false);
                    setIsListening(false);
                }
            };

            mediaRecorder.start();
            setTimeout(() => {
                if (mediaRecorder.state === 'recording') {
                    mediaRecorder.stop();
                }
            }, 3500);

        } catch (e) {
            console.error("Mic access denied:", e);
            setIsListening(false);
            alert("ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•´ìš”! ğŸ¤");
        }
    }, [setIsListening, setIsProcessingStt]);

    // âœ… ì˜ì¡´ì„± ë°°ì—´ì„ []ë¡œ ë§Œë“¤ì–´ ì™„ì „íˆ stableí•˜ê²Œ
    const startListening = useCallback(() => {
        // ì´ë¯¸ ë“£ê³  ìˆê±°ë‚˜ ì²˜ë¦¬ ì¤‘ì´ë©´ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
        if (isListeningRef.current || isProcessingSttRef.current || isStartingRef.current) return;

        // ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ í™•ì‹¤íˆ ì •ë¦¬
        if (recognitionRef.current) {
            recognitionRef.current.abort();
            recognitionRef.current = null;
        }

        isStartingRef.current = true;

        if ('webkitSpeechRecognition' in window) {
            const recognition = new (window as any).webkitSpeechRecognition();
            recognitionRef.current = recognition;

            recognition.lang = 'ko-KR';
            recognition.continuous = false;
            recognition.interimResults = false;
            recognition.maxAlternatives = 1;

            recognition.onstart = () => {
                console.log("ğŸ¤ Speech Recognition Started");
                isStartingRef.current = false;
                setIsListening(true);
            };

            recognition.onend = () => {
                console.log("ğŸ¤ Speech Recognition Ended");
                isStartingRef.current = false;
                // í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ê°€ ë‚´ ê²ƒì´ ë§ëŠ”ì§€ í™•ì¸
                if (recognitionRef.current === recognition) {
                    setIsListening(false);
                    recognitionRef.current = null;
                }
            };

            recognition.onresult = (event: any) => {
                const transcript = event.results[0][0].transcript;
                console.log("Mic Transcript:", transcript);
                const number = normalizeKoreanNumber(transcript);

                if (number) {
                    onResultRef.current(number);
                }
            };

            recognition.onerror = (event: any) => {
                console.error("Speech error:", event.error);
                isStartingRef.current = false;
                if (recognitionRef.current === recognition) {
                    setIsListening(false);
                    recognitionRef.current = null;
                }

                // 'not-allowed'ëŠ” ê¶Œí•œ ê±°ë¶€, 'no-speech'ëŠ” ì¹¨ë¬µ. 
                // ì¦‰ì‹œ í´ë°±ìœ¼ë¡œ ë„˜ì–´ê°€ë©´ ì‚¬ìš©ì ê²½í—˜ì´ ì•ˆ ì¢‹ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹ ì¤‘íˆ ì²˜ë¦¬
                if (event.error === 'not-allowed' || event.error === 'audio-capture') {
                    handleVoiceRecord();
                }
            };

            try {
                recognition.start();
            } catch (e) {
                console.error("Mic start error:", e);
                isStartingRef.current = false;
                setIsListening(false);
                // ì¦‰ì‹œ í´ë°± ì‹¤í–‰í•˜ì§€ ì•Šê³  ë©ˆì¶¤ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            }
        } else {
            handleVoiceRecord();
        }
    }, []); // âœ… ë¹ˆ ë°°ì—´ = ì ˆëŒ€ ì¬ìƒì„±ë˜ì§€ ì•ŠìŒ

    return {
        isListening: isListeningRef.current,
        isProcessingStt: isProcessingSttRef.current,
        startListening,
        stopListening
    };
};
