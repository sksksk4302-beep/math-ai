import { useState, useCallback, useRef, useEffect } from 'react';
import { normalizeKoreanNumber } from '../utils/korean';

interface UseSpeechRecognitionProps {
    onResult: (number: string) => void;
}

export const useSpeechRecognition = ({ onResult }: UseSpeechRecognitionProps) => {
    const [isListening, setIsListening] = useState(false);
    const [isProcessingStt, setIsProcessingStt] = useState(false);
    const streamRef = useRef<MediaStream | null>(null);
    const recognitionRef = useRef<any>(null);
    const onResultRef = useRef(onResult);

    useEffect(() => {
        onResultRef.current = onResult;
    }, [onResult]);

    // 1. ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ë¯¸ë¦¬ í™•ë³´ (Warm-up)
    useEffect(() => {
        const initStream = async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                streamRef.current = stream;
                console.log("ðŸŽ¤ Microphone stream initialized");
            } catch (e) {
                console.error("Microphone access denied or not available:", e);
            }
        };

        initStream();

        // ì–¸ë§ˆìš´íŠ¸ ì‹œ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        return () => {
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
                streamRef.current = null;
            }
            if (recognitionRef.current) {
                recognitionRef.current.abort();
                recognitionRef.current = null;
            }
        };
    }, []);

    const stopListening = useCallback(() => {
        if (recognitionRef.current) {
            recognitionRef.current.abort(); // stop() ëŒ€ì‹  abort()ê°€ ë” í™•ì‹¤í•˜ê²Œ ì¤‘ë‹¨
            recognitionRef.current = null;
        }
        setIsListening(false);
    }, []);

    const handleVoiceRecord = useCallback(async () => {
        setIsListening(true);
        try {
            // ìŠ¤íŠ¸ë¦¼ ìž¬ì‚¬ìš© ë˜ëŠ” ìƒˆë¡œ ìš”ì²­
            let stream = streamRef.current;
            if (!stream || !stream.active) {
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                streamRef.current = stream;
            }

            const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
            const audioChunks: Blob[] = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) audioChunks.push(event.data);
            };

            mediaRecorder.onstop = async () => {
                setIsProcessingStt(true);
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                const formData = new FormData();
                formData.append('file', audioBlob, 'recording.webm');

                try {
                    const apiUrl = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000';
                    const res = await fetch(`${apiUrl}/stt`, {
                        method: 'POST',
                        body: formData,
                        cache: 'no-store'
                    });
                    const data = await res.json();
                    if (data.number) {
                        onResultRef.current(data.number);
                    }
                } catch (e) {
                    console.error("STT Failed:", e);
                } finally {
                    setIsProcessingStt(false);
                    setIsListening(false);
                    // ìŠ¤íŠ¸ë¦¼ì„ ë‹«ì§€ ì•Šê³  ìœ ì§€í•¨ (ìž¬ì‚¬ìš© ìœ„í•´)
                }
            };

            mediaRecorder.start();

            setTimeout(() => {
                if (mediaRecorder.state === 'recording') {
                    mediaRecorder.stop();
                }
            }, 3500);

        } catch (e) {
            console.error("Mic access denied:", e);
            setIsListening(false);
            alert("ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•´ìš”! ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”. ðŸŽ¤");
        }
    }, []);

    const startListening = useCallback(() => {
        if (isListening || isProcessingStt) return;

        // ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬
        if (recognitionRef.current) {
            recognitionRef.current.abort();
        }

        if ('webkitSpeechRecognition' in window) {
            const recognition = new (window as any).webkitSpeechRecognition();
            recognitionRef.current = recognition;

            recognition.lang = 'ko-KR';
            recognition.continuous = false;
            recognition.interimResults = false;

            recognition.onstart = () => {
                setIsListening(true); // ëª…ì‹œì  ìƒíƒœ ì—…ë°ì´íŠ¸
            };

            recognition.onend = () => {
                // í˜„ìž¬ ì¸ìŠ¤í„´ìŠ¤ê°€ ë§žëŠ”ì§€ í™•ì¸ (Race Condition ë°©ì§€)
                if (recognitionRef.current === recognition) {
                    setIsListening(false);
                    recognitionRef.current = null;
                }
            };

            recognition.onresult = (event: any) => {
                const transcript = event.results[0][0].transcript;
                console.log("Mic Transcript:", transcript);

                const number = normalizeKoreanNumber(transcript);

                if (number) {
                    onResultRef.current(number);
                }
            };

            recognition.onerror = (event: any) => {
                console.error("Speech recognition error", event.error);

                // í˜„ìž¬ ì¸ìŠ¤í„´ìŠ¤ê°€ ë§žëŠ”ì§€ í™•ì¸
                if (recognitionRef.current === recognition) {
                    setIsListening(false);
                    recognitionRef.current = null;
                }

                if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                    handleVoiceRecord(); // í´ë°±
                }
            };

            // ì•½ê°„ì˜ ì§€ì—° í›„ ì‹œìž‘ (ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œê°„ í™•ë³´)
            setTimeout(() => {
                if (recognitionRef.current === recognition) {
                    try {
                        recognition.start();
                    } catch (e) {
                        console.error("Mic start error:", e);
                        setIsListening(false);
                        handleVoiceRecord();
                    }
                }
            }, 100);
        } else {
            handleVoiceRecord();
        }
    }, [isListening, isProcessingStt]);

    return {
        isListening,
        isProcessingStt,
        startListening,
        stopListening
    };
};
